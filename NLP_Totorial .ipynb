{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLMVJNJIN2sC",
        "outputId": "45411632-cd84-4fda-df5f-1574ad3a8896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import nltk.corpus"
      ],
      "metadata": {
        "id": "n9cWxVEufp40"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download specific corpora, such as the 'movie_reviews' corpus or any other\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')  # Example: download the 'punkt' tokenizer models\n",
        "nltk.download('stopwords')  # Download the stopwords corpus\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJoxJS6HgGaE",
        "outputId": "f91da011-c813-45ff-97ff-e22fc20c2c0e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C8ZXlwcgxdi",
        "outputId": "5f4f5308-f9e3-4f51-fddc-918112e15b73"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the default data path\n",
        "# print(nltk.data.path)"
      ],
      "metadata": {
        "id": "k2yV05Lsg0AK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.data.path.append('/path/to/your/nltk_data')\n",
        "\n",
        "# Check if 'corpora' exists in the NLTK data path\n",
        "# print(os.listdir(nltk.data.find('corpora').path))"
      ],
      "metadata": {
        "id": "9Ne0sgFrg9CT"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(os.listdir(nltk.data.find(\"corpora\").path))"
      ],
      "metadata": {
        "id": "WJmaKIvehMH-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(os.listdir(nltk.data.find(\"corpora\")))"
      ],
      "metadata": {
        "id": "tYkJgYxbhj0x"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir(nltk.data.find(\"corpora\")))"
      ],
      "metadata": {
        "id": "0A8g7Rqnhq3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1218724-811e-42e3-8b0d-f508ca38fe92"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['inaugural.zip', 'udhr2', 'brown_tei', 'senseval', 'switchboard', 'words.zip', 'masc_tagged.zip', 'pl196x', 'treebank', 'abc.zip', 'gazetteers', 'product_reviews_1.zip', 'sentiwordnet', 'nps_chat', 'english_wordnet', 'problem_reports.zip', 'switchboard.zip', 'dependency_treebank.zip', 'framenet_v17', 'timit.zip', 'ppattach', 'swadesh.zip', 'wordnet_ic.zip', 'ptb', 'propbank.zip', 'udhr2.zip', 'pe08.zip', 'smultron', 'alpino.zip', 'webtext.zip', 'ycoe.zip', 'cess_esp', 'treebank.zip', 'verbnet3.zip', 'chat80.zip', 'gutenberg', 'toolbox', 'shakespeare', 'subjectivity.zip', 'brown_tei.zip', 'indian.zip', 'genesis.zip', 'lin_thesaurus', 'machado.zip', 'ieer.zip', 'mte_teip5.zip', 'unicode_samples.zip', 'ycoe', 'framenet_v15', 'conll2002.zip', 'universal_treebanks_v20.zip', 'wordnet.zip', 'subjectivity', 'gazetteers.zip', 'floresta.zip', 'toolbox.zip', 'cmudict', 'kimmo', 'lin_thesaurus.zip', 'product_reviews_1', 'nombank.1.0.zip', 'wordnet2022.zip', 'comparative_sentences', 'pl196x.zip', 'conll2000.zip', 'sinica_treebank.zip', 'nps_chat.zip', 'pros_cons', 'smultron.zip', 'shakespeare.zip', 'paradigms', 'product_reviews_2.zip', 'city_database', 'dependency_treebank', 'jeita.zip', 'knbc.zip', 'chat80', 'mte_teip5', 'pe08', 'dolch', 'brown.zip', 'unicode_samples', 'product_reviews_2', 'crubadan.zip', 'twitter_samples', 'qc.zip', 'semcor.zip', 'brown', 'verbnet', 'europarl_raw', 'paradigms.zip', 'crubadan', 'pros_cons.zip', 'europarl_raw.zip', 'words', 'qc', 'rte', 'senseval.zip', 'reuters.zip', 'omw-1.4.zip', 'genesis', 'indian', 'biocreative_ppi', 'sentence_polarity', 'verbnet.zip', 'sentence_polarity.zip', 'movie_reviews.zip', 'state_union', 'movie_reviews', 'nonbreaking_prefixes', 'wordnet_ic', 'comparative_sentences.zip', 'extended_omw.zip', 'omw.zip', 'pil', 'webtext', 'verbnet3', 'rte.zip', 'opinion_lexicon', 'names', 'alpino', 'udhr.zip', 'wordnet2022', 'conll2000', 'state_union.zip', 'ppattach.zip', 'conll2007.zip', 'opinion_lexicon.zip', 'swadesh', 'nonbreaking_prefixes.zip', 'framenet_v15.zip', 'cmudict.zip', 'wordnet2021.zip', 'cess_cat.zip', 'cess_cat', 'udhr', 'inaugural', 'sentiwordnet.zip', 'conll2002', 'dolch.zip', 'problem_reports', 'comtrans.zip', 'english_wordnet.zip', 'twitter_samples.zip', 'city_database.zip', 'kimmo.zip', 'framenet_v17.zip', 'abc', 'stopwords.zip', 'stopwords', 'floresta', 'mac_morpho', 'pil.zip', 'timit', 'sinica_treebank', 'panlex_swadesh.zip', 'mac_morpho.zip', 'names.zip', 'cess_esp.zip', 'ieer', 'bcp47.zip', 'biocreative_ppi.zip', 'gutenberg.zip', 'ptb.zip', 'wordnet31.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "brown.words()"
      ],
      "metadata": {
        "id": "Pj3vCM5bjW07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f87da0af-184f-4370-9353-e52fa6dcf53f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "metadata": {
        "id": "moRTbadLkfDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ae23fe-2115-4697-b6fa-aa8a43bf66f7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we take the shakespeare-hamlet.text\n",
        "hamlet=nltk.corpus.gutenberg.words(\"shakespeare-hamlet.txt\")\n",
        "hamlet"
      ],
      "metadata": {
        "id": "Cv7E-Hxx2bgm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2fcf3d4-7f0b-4215-bcc5-059ad70a0db5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in hamlet[:500]: # text generatated in 500 words\n",
        "  print(word,sep='',end='')"
      ],
      "metadata": {
        "id": "Jxa_jJo1KI_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e94b038-7c64-4926-fcd6-db610c2638bc"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TheTragedieofHamletbyWilliamShakespeare1599]ActusPrimus.ScoenaPrima.EnterBarnardoandFranciscotwoCentinels.Barnardo.Who'sthere?Fran.Nayanswerme:Stand&vnfoldyourselfeBar.LongliuetheKingFran.Barnardo?Bar.HeFran.YoucomemostcarefullyvponyourhoureBar.'Tisnowstrooktwelue,gettheetobedFranciscoFran.Forthisreleefemuchthankes:'Tisbittercold,AndIamsickeatheartBarn.HaueyouhadquietGuard?Fran.NotaMousestirringBarn.Well,goodnight.IfyoudomeetHoratioandMarcellus,theRiualsofmyWatch,bidthemmakehast.EnterHoratioandMarcellus.Fran.IthinkeIhearethem.Stand:who'sthere?Hor.FriendstothisgroundMar.AndLeige-mentotheDaneFran.GiueyougoodnightMar.OfarwelhonestSoldier,whohathrelieu'dyou?Fra.Barnardoha'smyplace:giueyougoodnight.ExitFran.Mar.HollaBarnardoBar.Say,whatisHoratiothere?Hor.ApeeceofhimBar.WelcomeHoratio,welcomegoodMarcellusMar.What,ha'sthisthingappear'dagainetonightBar.IhaueseenenothingMar.Horatiosaies,'tisbutourFantasie,AndwillnotletbeleefetakeholdofhimTouchingthisdreadedsight,twiceseeneofvs,ThereforeIhaueintreatedhimalongWithvs,towatchtheminutesofthisNight,ThatifagainethisApparitioncome,Hemayapproueoureyes,andspeaketoitHor.Tush,tush,'twillnotappeareBar.Sitdownea-while,Andletvsonceagaineassaileyoureares,ThataresofortifiedagainstourStory,WhatwetwoNightshaueseeneHor.Well,sitwedowne,AndletvsheareBarnardospeakeofthisBarn.Lastnightofall,WhenyondsameStarrethat'sWestwardfromthePoleHadmadehiscourset'illumethatpartofHeauenWherenowitburnes,Marcellusandmyselfe,TheBellthenbeatingoneMar.Peace,breaketheeof:EntertheGhost.LookewhereitcomesagaineBarn.Inthesamefigure,liketheKingthat'sdeadMar.ThouartaScholler;speaketoitHoratioBarn.LookesitnotliketheKing?MarkeitHoratioHora.Mostlike:Itharrowesmewithfear&wonderBarn.ItwouldbespoketooMar.QuestionitHoratioHor.Whatart"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### We can take the owne paragraph about AI:"
      ],
      "metadata": {
        "id": "4CW8hBuUOtNs"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AI='''Artificial Intelligence (AI) is a branch of computer science that focuses on creating machines and software that can perform tasks typically requiring human intelligence. These tasks include learning, reasoning, problem-solving, understanding natural language, recognizing images, and making decisions. AI systems can be trained on large amounts of data to identify patterns and improve over time, a process known as machine learning. Advanced applications of AI are found in virtual assistants, recommendation systems, self-driving cars, robotics, and healthcare. By mimicking the way humans think and learn, AI is transforming industries and becoming an essential part of modern technology'''"
      ],
      "metadata": {
        "id": "6hwTHoB6M2Tn"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(AI) # Check the AI type"
      ],
      "metadata": {
        "id": "p-f6ZdBAOQ57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3e6585-d88f-4428-97ff-cdd9fa2983ef"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## import the tokenzie we use tokenzie the paragraph is conveted into tokenes\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "VREoaf3POUL5"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AI_tokens=word_tokenize(AI)\n",
        "AI_tokens"
      ],
      "metadata": {
        "id": "TeqMIrYQObS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b875504-d258-4e14-c3f6-7249211cf69a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Artificial',\n",
              " 'Intelligence',\n",
              " '(',\n",
              " 'AI',\n",
              " ')',\n",
              " 'is',\n",
              " 'a',\n",
              " 'branch',\n",
              " 'of',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'that',\n",
              " 'focuses',\n",
              " 'on',\n",
              " 'creating',\n",
              " 'machines',\n",
              " 'and',\n",
              " 'software',\n",
              " 'that',\n",
              " 'can',\n",
              " 'perform',\n",
              " 'tasks',\n",
              " 'typically',\n",
              " 'requiring',\n",
              " 'human',\n",
              " 'intelligence',\n",
              " '.',\n",
              " 'These',\n",
              " 'tasks',\n",
              " 'include',\n",
              " 'learning',\n",
              " ',',\n",
              " 'reasoning',\n",
              " ',',\n",
              " 'problem-solving',\n",
              " ',',\n",
              " 'understanding',\n",
              " 'natural',\n",
              " 'language',\n",
              " ',',\n",
              " 'recognizing',\n",
              " 'images',\n",
              " ',',\n",
              " 'and',\n",
              " 'making',\n",
              " 'decisions',\n",
              " '.',\n",
              " 'AI',\n",
              " 'systems',\n",
              " 'can',\n",
              " 'be',\n",
              " 'trained',\n",
              " 'on',\n",
              " 'large',\n",
              " 'amounts',\n",
              " 'of',\n",
              " 'data',\n",
              " 'to',\n",
              " 'identify',\n",
              " 'patterns',\n",
              " 'and',\n",
              " 'improve',\n",
              " 'over',\n",
              " 'time',\n",
              " ',',\n",
              " 'a',\n",
              " 'process',\n",
              " 'known',\n",
              " 'as',\n",
              " 'machine',\n",
              " 'learning',\n",
              " '.',\n",
              " 'Advanced',\n",
              " 'applications',\n",
              " 'of',\n",
              " 'AI',\n",
              " 'are',\n",
              " 'found',\n",
              " 'in',\n",
              " 'virtual',\n",
              " 'assistants',\n",
              " ',',\n",
              " 'recommendation',\n",
              " 'systems',\n",
              " ',',\n",
              " 'self-driving',\n",
              " 'cars',\n",
              " ',',\n",
              " 'robotics',\n",
              " ',',\n",
              " 'and',\n",
              " 'healthcare',\n",
              " '.',\n",
              " 'By',\n",
              " 'mimicking',\n",
              " 'the',\n",
              " 'way',\n",
              " 'humans',\n",
              " 'think',\n",
              " 'and',\n",
              " 'learn',\n",
              " ',',\n",
              " 'AI',\n",
              " 'is',\n",
              " 'transforming',\n",
              " 'industries',\n",
              " 'and',\n",
              " 'becoming',\n",
              " 'an',\n",
              " 'essential',\n",
              " 'part',\n",
              " 'of',\n",
              " 'modern',\n",
              " 'technology']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lenght of tokens\n",
        "len(AI_tokens) # Number of Tokens"
      ],
      "metadata": {
        "id": "lXPp2eCbOmJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86217dc9-533e-40ea-e9d8-5c41ab03a5c8"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To analyze text frequency (how often a word appears).To find the most common words in a document.To visualize word frequency with charts.\n",
        "from nltk.probability import FreqDist\n",
        "fdist=FreqDist()"
      ],
      "metadata": {
        "id": "J_LkbwoqPbe0"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in  AI_tokens:\n",
        "   fdist[word.lower()]+=1 # in nltk Lowercase words number how many time's repeted is visual's\n",
        "fdist\n"
      ],
      "metadata": {
        "id": "Veserz76QK30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5270ad0a-3a5d-4ad4-a4c7-9867fa5e7b27"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 11, 'and': 6, 'ai': 4, 'of': 4, '.': 4, 'intelligence': 2, 'is': 2, 'a': 2, 'that': 2, 'on': 2, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find the Frequency of artificial: how to find >>>? how many times repet the word that is the frequency\n",
        "fdist['artificial']"
      ],
      "metadata": {
        "id": "mzN9nu85Qpdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7a3a744-2e98-4c20-a3f3-3085e2a18a72"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist['is']"
      ],
      "metadata": {
        "id": "u4FAFwIbRvR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a82aa4-2d8a-490f-8af0-4d62c710c41c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist[\",\"]"
      ],
      "metadata": {
        "id": "OZPidoLpStPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e5c3d1-27a0-4e9f-a9a6-753d934fbdd0"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(fdist)"
      ],
      "metadata": {
        "id": "3zeWKil8SzEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9381062-9806-47e1-c879-a6f8e2bd0e4d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_top10 = fdist.most_common(10)\n",
        "fdist_top10"
      ],
      "metadata": {
        "id": "VH8ZShAYS3Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e62480-527f-489e-e2b1-879e58484fdb"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 11),\n",
              " ('and', 6),\n",
              " ('ai', 4),\n",
              " ('of', 4),\n",
              " ('.', 4),\n",
              " ('intelligence', 2),\n",
              " ('is', 2),\n",
              " ('a', 2),\n",
              " ('that', 2),\n",
              " ('on', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#blankline tokenize used to split text into paragraphs by looking for blank lines.\n",
        "\n",
        "\n",
        "from nltk.tokenize import blankline_tokenize\n",
        "AI_blank=blankline_tokenize(AI)\n",
        "len(AI_blank)"
      ],
      "metadata": {
        "id": "BLouaVapaeiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8993ccfd-fb4f-403d-ae10-46c8a0f0df56"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AI_blank[0]\n",
        "# if len(AI_blank) > 2:\n",
        "#     print(AI_blank[2])\n",
        "# else:\n",
        "#     print(\"Element at index 2 does not exist.\")\n"
      ],
      "metadata": {
        "id": "SWVOqvCkbaTe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "d5149689-3d9d-45fc-87f0-ee7d862fbe09"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Artificial Intelligence (AI) is a branch of computer science that focuses on creating machines and software that can perform tasks typically requiring human intelligence. These tasks include learning, reasoning, problem-solving, understanding natural language, recognizing images, and making decisions. AI systems can be trained on large amounts of data to identify patterns and improve over time, a process known as machine learning. Advanced applications of AI are found in virtual assistants, recommendation systems, self-driving cars, robotics, and healthcare. By mimicking the way humans think and learn, AI is transforming industries and becoming an essential part of modern technology'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import bigrams,trigrams,ngrams"
      ],
      "metadata": {
        "id": "8MjJ-J8ifwRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import bigrams,trigrams,ngrams"
      ],
      "metadata": {
        "id": "JxBPDOXpej8E"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str=\"The best and most beautifull things in the world cannot be seen or even touched,they must be first with the heart\"\n",
        "quotes_tokens=nltk.word_tokenize(str)\n",
        "quotes_tokens\n",
        "#  word_tokenize >function to split the quote into individual words and punctuation tokens.\n",
        "# It returns a list of tokens (words, punctuation, etc.).\n",
        "#  and it Handle the punctuation ,contractions and sentence rules\n"
      ],
      "metadata": {
        "id": "e_Jt0kEygCCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb397bb-b21c-4c98-caf9-2fa9030cc999"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'best',\n",
              " 'and',\n",
              " 'most',\n",
              " 'beautifull',\n",
              " 'things',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " 'can',\n",
              " 'not',\n",
              " 'be',\n",
              " 'seen',\n",
              " 'or',\n",
              " 'even',\n",
              " 'touched',\n",
              " ',',\n",
              " 'they',\n",
              " 'must',\n",
              " 'be',\n",
              " 'first',\n",
              " 'with',\n",
              " 'the',\n",
              " 'heart']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  bigram is simply a pair of two consecutive words in a sequence. like before=(i love nltk)>> do bigrams after=(i love),(love nltk)\n",
        "quotes_bigrams= list(nltk.bigrams(quotes_tokens))\n",
        "quotes_bigrams"
      ],
      "metadata": {
        "id": "nwy04ckGg0qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412cf7ff-4a73-4149-c5c5-8c8953d3f03b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'best'),\n",
              " ('best', 'and'),\n",
              " ('and', 'most'),\n",
              " ('most', 'beautifull'),\n",
              " ('beautifull', 'things'),\n",
              " ('things', 'in'),\n",
              " ('in', 'the'),\n",
              " ('the', 'world'),\n",
              " ('world', 'can'),\n",
              " ('can', 'not'),\n",
              " ('not', 'be'),\n",
              " ('be', 'seen'),\n",
              " ('seen', 'or'),\n",
              " ('or', 'even'),\n",
              " ('even', 'touched'),\n",
              " ('touched', ','),\n",
              " (',', 'they'),\n",
              " ('they', 'must'),\n",
              " ('must', 'be'),\n",
              " ('be', 'first'),\n",
              " ('first', 'with'),\n",
              " ('with', 'the'),\n",
              " ('the', 'heart')]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A trigram is a sequence of three consecutive words from a text.\n",
        "quotes_trigrams= list(nltk.trigrams(quotes_tokens))\n",
        "quotes_trigrams\n",
        "# like before  \"I love learning NLP\" after trigram==[(\"I\", \"love\", \"learning\"), (\"love\", \"learning\", \"NLP\")]\n",
        "\n"
      ],
      "metadata": {
        "id": "-Nx7wPGDiEQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3f0994-5d32-4574-c05c-6df9e66b5b45"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'best', 'and'),\n",
              " ('best', 'and', 'most'),\n",
              " ('and', 'most', 'beautifull'),\n",
              " ('most', 'beautifull', 'things'),\n",
              " ('beautifull', 'things', 'in'),\n",
              " ('things', 'in', 'the'),\n",
              " ('in', 'the', 'world'),\n",
              " ('the', 'world', 'can'),\n",
              " ('world', 'can', 'not'),\n",
              " ('can', 'not', 'be'),\n",
              " ('not', 'be', 'seen'),\n",
              " ('be', 'seen', 'or'),\n",
              " ('seen', 'or', 'even'),\n",
              " ('or', 'even', 'touched'),\n",
              " ('even', 'touched', ','),\n",
              " ('touched', ',', 'they'),\n",
              " (',', 'they', 'must'),\n",
              " ('they', 'must', 'be'),\n",
              " ('must', 'be', 'first'),\n",
              " ('be', 'first', 'with'),\n",
              " ('first', 'with', 'the'),\n",
              " ('with', 'the', 'heart')]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_ngrams= list(nltk.ngrams(quotes_tokens,5))\n",
        "quotes_ngrams\n",
        "#  same to bigrams and trigrams but we are given ANY NUMBER ,that number of list \"ngrams\" is presented"
      ],
      "metadata": {
        "id": "SZ_ulH7VjBTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639134f7-fc51-460b-f0c9-fb102fa1f7c3"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'best', 'and', 'most', 'beautifull'),\n",
              " ('best', 'and', 'most', 'beautifull', 'things'),\n",
              " ('and', 'most', 'beautifull', 'things', 'in'),\n",
              " ('most', 'beautifull', 'things', 'in', 'the'),\n",
              " ('beautifull', 'things', 'in', 'the', 'world'),\n",
              " ('things', 'in', 'the', 'world', 'can'),\n",
              " ('in', 'the', 'world', 'can', 'not'),\n",
              " ('the', 'world', 'can', 'not', 'be'),\n",
              " ('world', 'can', 'not', 'be', 'seen'),\n",
              " ('can', 'not', 'be', 'seen', 'or'),\n",
              " ('not', 'be', 'seen', 'or', 'even'),\n",
              " ('be', 'seen', 'or', 'even', 'touched'),\n",
              " ('seen', 'or', 'even', 'touched', ','),\n",
              " ('or', 'even', 'touched', ',', 'they'),\n",
              " ('even', 'touched', ',', 'they', 'must'),\n",
              " ('touched', ',', 'they', 'must', 'be'),\n",
              " (',', 'they', 'must', 'be', 'first'),\n",
              " ('they', 'must', 'be', 'first', 'with'),\n",
              " ('must', 'be', 'first', 'with', 'the'),\n",
              " ('be', 'first', 'with', 'the', 'heart')]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "tAyuYhAekNZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pst=PorterStemmer()\n",
        "# Posterstemmer:> Its used to reduce words to their root form (or \"stem\"), by removing common suffixes like:\n",
        "\n",
        "# /like=ing, ly, ed, s, etc.\n"
      ],
      "metadata": {
        "id": "z4F_pXyQjWVJ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pst.stem(\"having\")"
      ],
      "metadata": {
        "id": "bgqd-mIWkeYr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "575956cb-3890-4aac-ab73-f785f6fba76f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pst.stem( \"runs\")\n"
      ],
      "metadata": {
        "id": "WaskSKKmkqt3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ec9f9e3f-88f5-4b0e-ea3a-d9062e1bd5ce"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'run'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pst.stem(\"beatufully\")"
      ],
      "metadata": {
        "id": "nrdmVqo4le2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9ed6d5fd-a3be-422e-8540-805ec0333564"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'beatu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_stee=['give','given','givening','gave']\n",
        "for words in words_to_stee:\n",
        "  print(words+ ','+pst.stem(words))"
      ],
      "metadata": {
        "id": "SJueMiX2mEGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba31105-7bb5-466d-a532-3ee091983e93"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give,give\n",
            "given,given\n",
            "givening,given\n",
            "gave,gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer \t# LancasterStemmer is NLP stemmer that removes prefixes/suffixes aggressively\n",
        "lst=LancasterStemmer()\n",
        "for words in words_to_stee:\n",
        "  # print(lst+','+lst.stem(lst))\n",
        "  print(words+ ','+lst.stem(words))"
      ],
      "metadata": {
        "id": "PpxASs5joV9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16571a70-725c-4cef-9b1a-82707d61a5f6"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give,giv\n",
            "given,giv\n",
            "givening,giv\n",
            "gave,gav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "sbst=SnowballStemmer('english')\n",
        "# SnowballStemmer>>Its available in the NLTK library and supports multiple languages"
      ],
      "metadata": {
        "id": "egQGtrJ-sQL4"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for words in words_to_stee:\n",
        "  print(words+ ','+sbst.stem(words))"
      ],
      "metadata": {
        "id": "WB7_HfsUzCcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf7cfbcc-fda6-453a-fea8-105fe2c93024"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give,give\n",
            "given,given\n",
            "givening,given\n",
            "gave,gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "##### Lemmatization is the process of reducing a word to its base or dictionary form, called a lemma.Unlike stemming, which just chops off word endings, lemmatization uses context and grammar rules to find the correct base word."
      ],
      "metadata": {
        "id": "jx4rK2XiIH9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word_Lee=WordNetLemmatizer() # Lemmatization is the dictionary form word"
      ],
      "metadata": {
        "id": "XNUHGeqTzQjh"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for words in words_to_stee:\n",
        "  print(words+':'+word_Lee.lemmatize(words))"
      ],
      "metadata": {
        "id": "scYkoLV5JQKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09795db5-ff1e-40c8-d2d2-82b71b13772f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give:give\n",
            "given:given\n",
            "givening:givening\n",
            "gave:gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StopWords\n",
        "##### Stopwords are the most common words in a language that are usually filtered out before processing text because they dont carry significant meaning.\n",
        "##### like>>.\"the\", \"is\", \"in\", \"and\", \"to\", \"it\", \"of\", \"a\", \"on\", \"this\", \"that\", \"are\"\n"
      ],
      "metadata": {
        "id": "dw8f6AxEKubo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# thr stop words can't help to the NLTK\n",
        "from nltk.corpus import stopwords\n",
        "# mention the laguage\n",
        "stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "1AwQ5QviKH93",
        "outputId": "20eb511c-c5ab-4469-ad2d-6fac379e19dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "Y7NsOWjXMAFi",
        "outputId": "ad3a6384-3366-47f8-ec2d-e264c036a0c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "198"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_top10"
      ],
      "metadata": {
        "id": "4RBTaFwKMH-3",
        "outputId": "e6f3943e-98f0-481a-9842-3298fe48061c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 11),\n",
              " ('and', 6),\n",
              " ('ai', 4),\n",
              " ('of', 4),\n",
              " ('.', 4),\n",
              " ('intelligence', 2),\n",
              " ('is', 2),\n",
              " ('a', 2),\n",
              " ('that', 2),\n",
              " ('on', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Punctuation\n",
        "##### Punctuation refers to symbols used in writing to separate sentences and clarify meaning.Symbols that structure written text the nltk is remove the to clean text"
      ],
      "metadata": {
        "id": "-mGvKRzyMlCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "punctuation=re.compile(r'[-.?!.:;()[0-9]]')## removies the commas(,) & - & _ like sybomes is removed"
      ],
      "metadata": {
        "id": "yCh_3sDCMQTV"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_punctuation=[]\n",
        "for worda in AI_tokens:\n",
        "  word=punctuation.sub(\"\",words)\n",
        "  if len(word)>0:\n",
        "    post_punctuation.append(word)"
      ],
      "metadata": {
        "id": "elK_b_AyNRBM"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_punctuation"
      ],
      "metadata": {
        "id": "0gJFcORnODv9",
        "outputId": "8518a4d8-9038-46d3-c421-8476b286a53b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave',\n",
              " 'gave']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part of Speech(pos)\n",
        "###### Part of Speech (POS) refers to the grammatical category of a word based on its role in a sentence.It tells us how a word functions  like whether it's a noun, verb, adjective, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "9-4Ob8W0Oxqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent='Timothy is a natural when it comes to drawing'\n",
        "sent_tokens=word_tokenize(sent)"
      ],
      "metadata": {
        "id": "RQfxQO2kOJ71"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in sent_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "id": "62wRqh2dPoV3",
        "outputId": "802e309d-977d-4729-c279-7ab271fac3ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Timothy', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('a', 'DT')]\n",
            "[('natural', 'JJ')]\n",
            "[('when', 'WRB')]\n",
            "[('it', 'PRP')]\n",
            "[('comes', 'VBZ')]\n",
            "[('to', 'TO')]\n",
            "[('drawing', 'VBG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS Tag's"
      ],
      "metadata": {
        "id": "m-SZBetoXHo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ex=2\n",
        "sent1=\"raki is eating a delidious cake\"\n",
        "sent1_tokens=word_tokenize(sent1)\n",
        "for token in sent1_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "id": "qEilQXWtP1Kh",
        "outputId": "a011d0d4-8df9-4a41-be2c-878c6c6cefc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('raki', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('eating', 'VBG')]\n",
            "[('a', 'DT')]\n",
            "[('delidious', 'JJ')]\n",
            "[('cake', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Name Entity Recognition(NER)\n",
        "###### NER is the process of identifying and classifying named entities in text into predefined categories like:"
      ],
      "metadata": {
        "id": "dQA017OfQq6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The \"Name Entity Recognition \"is do..like this>> identify the person , location,Organization ,Date,money ect>>\n",
        "from nltk import ne_chunk\n",
        "#chunk is >>>>Its a function used to identify named entities (like people, places, organizations) in a parsed sentence, and group them into chunks (trees)."
      ],
      "metadata": {
        "id": "-f9_3KhqQf0P"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NE_sent=\"The US President stays in the WHITE HOUSE\"\n"
      ],
      "metadata": {
        "id": "YSr8f0V-SpUT"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NE_token=word_tokenize(NE_sent)\n",
        "NE_tags=nltk.pos_tag(NE_token)"
      ],
      "metadata": {
        "id": "PTvFHK05SqIU"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NE_NER=ne_chunk(NE_tags)\n",
        "print(NE_NER)"
      ],
      "metadata": {
        "id": "VIAJde9QS5j3",
        "outputId": "b1f15e77-dbbe-425c-aa88-693c11f1c179",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (ORGANIZATION US/NNP)\n",
            "  President/NNP\n",
            "  stays/VBZ\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (FACILITY WHITE/NNP HOUSE/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking:\n",
        "#### The chunking is the picking up individual pieces of information and Grouping them into bigger pieces"
      ],
      "metadata": {
        "id": "ewpzckQjTr4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new=\"The big cat at the little mouse who was after fresh chese\"\n",
        "new_tokens=nltk.pos_tag(word_tokenize(new))\n",
        "new_tokens"
      ],
      "metadata": {
        "id": "6awfFxBLTEiT",
        "outputId": "63acda1c-7b7c-4489-e352-2680c57c2368",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('big', 'JJ'),\n",
              " ('cat', 'NN'),\n",
              " ('at', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('little', 'JJ'),\n",
              " ('mouse', 'NN'),\n",
              " ('who', 'WP'),\n",
              " ('was', 'VBD'),\n",
              " ('after', 'IN'),\n",
              " ('fresh', 'JJ'),\n",
              " ('chese', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## defined the grammar\n",
        "grammar_np=r'NP:{<DT>?<JJ>*<NN>}'"
      ],
      "metadata": {
        "id": "YwSzqyP8XCG-"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_parser = nltk.RegexpParser(grammar_np)\n"
      ],
      "metadata": {
        "id": "T8Vt2ND4VyTs"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create Chunk Parser\n",
        "chunk_parser = nltk.RegexpParser(grammar_np)\n",
        "\n",
        "#  Parse the sentence\n",
        "chunk_result = chunk_parser.parse(new_tokens)\n",
        "\n",
        "#  Print the result\n",
        "print(chunk_result)"
      ],
      "metadata": {
        "id": "24v78fqUWU4p",
        "outputId": "1d36f0a9-61cb-497b-8275-6a40870a96d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP The/DT big/JJ cat/NN)\n",
            "  at/IN\n",
            "  (NP the/DT little/JJ mouse/NN)\n",
            "  who/WP\n",
            "  was/VBD\n",
            "  after/IN\n",
            "  (NP fresh/JJ chese/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzLzpY0FWn_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}